\chapter{Comparison}

In the evaluation phase of the work we concentrated on the criteria listed below.

\begin{description}

  \item[Run-time Performance]~\\[0.1cm]
  We used each of the parsing algorithms from the implementation phase and executed them using different input files. In each instance we measured the time it took to finish the parsing process. We repeated this multiple times for each combination of parsing technique and file, so we could determine median execution times. At the end of the phase we determined the fastest parsing code and therefore also answered~\Cref{que:speed}:

  \speed*

  \item[Memory Usage]~\\[0.1cm]
  We determined the memory usage of the different parsing implementation using \href{http://valgrind.org}{Valgrind's} heap profiler \href{http://valgrind.org/docs/manual/ms-manual.html}{Massif}.

  \item[Code Size]~\\[0.1cm]
  We counted the lines of the different implementations using the tool \href{https://github.com/AlDanial/cloc}{cloc}.

  \item[Code Complexity]~\\[0.1cm]
  For the measurement of the code complexity we determined
  \begin{itemize}
    \item the cyclomatic complexity~\cite{mccabe1976complexity}, and
    \item the Halstead complexity measures~\cite{halstead1977elements}
  \end{itemize}
  of the generated code.

  \item[Ease of Extensibility and Composability]~\\[0.1cm]
  Since there is no common way to measure either of this attributes we only looked into them from a more informal point of view. In this part of the evaluation we describe some of the features we found that hinder or enhance the extensibility and composability of the used parsing methods. At this stage we also answer~\Cref{que:closeness}:

  \closeness*

  , and argue why certain parsing libraries allow us to stay closer to the definition of the configuration language.

  \item[Error Reporting]~\\[0.1cm]
  For this subtask we created YAML input files, that contain certain errors. We defined reference error messages that describe the problem in a way that is as user-friendly as possible. We then looked how far we can approximate the ideal messages with the different parsing methods.

  \item[Security Problems]~\\[0.1cm]
  We used a \href{https://en.wikipedia.org/wiki/Fuzzing}{fuzzer} (\href{http://lcamtuf.coredump.cx/afl}{american fuzzy lop}) to test the quality of the generated parsing code.

\end{description}

\section{Error Reporting}

\emph{Error handling} can be grouped into three dependent stages~\cite{ruefenacht2016error}:

\begin{enumerate}
  \item Error Detection
  \item Error Recovery
  \item Error Correction
\end{enumerate}

. We are mainly concerned with error detection in this thesis, since error recovery and error correction are generally not possible without the possibility of fixing errors incorrectly. These errors can be disastrous in case an important configuration value, such as “radiation intensity” is set incorrectly as a result of error correction.

While there exist techniques to enhance error reporting using external tools or modifying a parser engine~\cite{jeffery2003generating, cox2010errors}, we will only consider built-in solutions or slight modifications to a grammar. We do this, since extending a parser engine is out of scope of the thesis and elaborate extensions would also make the comparison concerning error reporting unfair.

\section{Unmodified Error Messages}

\begin{yamlcode}
  key:
   - element 1
  - element 2
\end{yamlcode}
