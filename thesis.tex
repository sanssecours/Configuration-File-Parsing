%!TEX TS-program = xelatex

\documentclass[draft, oneside, final]{vutinfth}

% -- Packages ------------------------------------------------------------------

\usepackage{fontspec}
\usepackage[sorting=ynt, style=alphabetic, backend=biber]{biblatex}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{microtype}
\usepackage{nag}
\usepackage{titlesec}
\usepackage{xcolor}
\usepackage{xelatexemoji}
\usepackage{hyperref}
\usepackage[acronym, toc]{glossaries}

% -- Attributes ----------------------------------------------------------------

\newcommand{\authorname}{René Schwaiger}
\newcommand{\thesistitle}{Parsing of Configuration Files}

\setauthor{}{\authorname}{}{male}
\setadvisor{Ao.Univ.Prof. Dipl.-Ing. Dr.techn.}{Franz Puntigam}{}{male}
\setfirstassistant{Markus Raab}{Markus Raab}{}{male}

\setaddress{Waldmüllergasse 9}
\setregnumber{0425176}
\setdate{\day}{\month}{\year}
\settitle{\thesistitle}{Parsing von Konfigurationsdateien}

\setthesis{master}
\setmasterdegree{dipl.}

\setcurriculum{Computer Engineering}{Technische Informatik}

% -- Settings ------------------------------------------------------------------

% Bibliography
\addbibresource{References.bib}

% Colors
\definecolor{Red}{rgb}{0.84, 0.15, 0.19}
\definecolor{Blue}{rgb}{0, 0.41, 0.60}

% Fonts
\setmainfont[Mapping=tex-text]{Seravek}
\setsansfont[Mapping=tex-text]{Ubuntu}
\setmonofont[Scale=MatchLowercase]{Menlo}

% Hyperref
\hypersetup{
  pdfauthor = {\authorname},
  pdftitle = {\thesistitle},
  pdfsubject = {Comparison of Different Parsing Methods},
  pdfkeywords = {Parsing, Configuration, Elektra},
  colorlinks=true,
  linkcolor=black,
  anchorcolor=black,
  citecolor=Red,
  urlcolor=Red
}

% Glossary
\makeindex
\makeglossaries

% Indentation & Paragraphs
\nonzeroparskip
\setlength{\parindent}{0pt}

% Page Numbering
\renewcommand{\pagenumbering}[1]{}

% Section & Paragraph Style
\titleformat{\section}{\large\sffamily\bfseries}{}{0pt}{\thesection~}
  [{\color{Red}\hrule}]

% -- Glossary ------------------------------------------------------------------

\newacronym{ANTLR}{ANTLR}{Another Tool for Language Recognition}
\newacronym{KDB}{KDB}{key database}
\newacronym{PEG}{PEG}{Parsing Expression Grammar}
\newacronym{YAML}{YAML}{YAML Ain't Markup Language}

\newcommand{\GlsShort}[1]
  {\setacronymstyle{short-long}\gls{#1}\setacronymstyle{long-short}}

% -- Document ------------------------------------------------------------------

\begin{document}

\frontmatter

\addtitlepage{naustrian}
\addtitlepage{english}
\addstatementpage

\selectlanguage{english}

\tableofcontents

\mainmatter

\chapter{Introduction}

\section{Motivation \& Problem Statement}
\label{sec:Motivation}

\begin{sloppypar}
\emph{Parsing} is the process of taking input and converting it into a data structure~\cite{wikipedia2016Parser, grune2007parsing}. Research around this topic focuses on how to process the input and the general computational complexity of algorithms to handle certain kind of \emph{formal languages}.
\end{sloppypar}

This thesis concerns itself with the parsing process of languages that are able to express configuration data (e.g. INI, TOML, YAML). These languages form an interesting subpart of formal languages, since most computer programs only store and access key-value based persistent configuration data.

Just like people disagree about the “best” configuration format, there is currently no consensus, as to which is the ideal way to parse configuration data. There are many possible ways to parse and store data. Notable examples include:

\begin{itemize}
  \item Bidirectional programming~\cite{foster2005combinators, bohannon2006relational, lutterkort2008augeas, ko2016bigul, raab2016improving}
  \item Code produced by a parser generator~\cite{denny2008ielr, parr2014adaptive, warth2016modular, bates2017aprt}
  \item Serialization libraries~\cite{sumaray2012cds, pacini2015performance}
  \item Hand-written parsers~\cite{myers2008cparser, bendersky2012clang}
\end{itemize}

Currently the possibility to compare different parsing techniques is limited. The naive approach would be to just run different parsers on the same data. In practice however, this approach is not usable, since parser tools tend to produce very different data structures. Some of them do not produce data structures at all, instead they let the user specify subroutines that should be called when the parser matches parts of the grammar.

As part of this thesis we will tackle this problem, by using different parsing techniques within a common configuration framework. This integration eliminates the problem of comparing the parsing process under different circumstances, since the data structures the parsers create will always be the same. In this novel approach we will use \href{http://web.libelektra.org}{Elektra}, a key-value database, as configuration framework. Elektra’s storage plugin interface will act as foundation for the parsing process. In the end the thesis should provide answers about which parsing techniques provide an ideal balance between performance and usability.

\section{Aim of the Work}
\label{sec:Aim_Of_The_Work}

Elektra~\cite{raab2010modular} is a plugin based framework that stores configuration parameters in a \glsdesc{KDB}. Elektra reads and stores configuration data via so-called \emph{storage plugins}. In this thesis we will compare various ways of parsing by writing and generating parsing code for different storage plugins.

For this purpose we wrote a storage plugin that parses a minimal subset of \glstext{YAML}, a human readable configuration language. We started by writing new parsing code using different techniques. The parsing code is configurable at compile time. We used the following parsing technologies:

\begin{itemize}
  \item Handwritten Parser (Recursive Descent)
  \item ALL(*)-Parser Generator (\href{http://www.antlr.org}{ANTLR})
  \item LR-Parser Generator (\href{https://www.gnu.org/software/bison}{GNU Bison})
  \item Earley Parser (\href{https://github.com/vnmakarov/yaep}{yaep})
  \item PEG-Parser (\href{https://github.com/ColinH/PEGTL}{PEGTL})
  \item Parser Combinator (\href{https://github.com/orangeduck/mpc}{mpc})
  \item Bidirectional Programming (\href{http://augeas.net}{Augeas})
\end{itemize}

We compared the parsing code according to the following criteria:

\begin{itemize}
  \item Run-Time Performance
  \item Memory Usage
  \item Code Size
  \item Overall Code Complexity
  \item Ease of Extensibility and Composability
  \item Error Reporting
\end{itemize}

After we finished the parsing code and comparison of the minimal (but challenging) YAML subset, we wrote a more complete \glstext{YAML} parser using the most promising technique.

In the scope of the above comparison we answered the following questions:

\begin{itemize}[label=❓]

  \item Which parsing technique allows us to stay closest to the definition of the configuration language?

  \item Are hand-written parsers faster, than the ones generated by a parser generator?

\end{itemize}

\section{Methodological Approach}

The methodological approach for this thesis consists of the following steps:

\begin{description}[style=multiline, leftmargin=3.4cm, font=\bfseries]

  \item[Literature Review] We determined the current status of parsing libraries suitable for configuration file parsing. After that we chose appropriate libraries for the parsing techniques listed in the section “\nameref{sec:Aim_Of_The_Work}”. In this part of the thesis we also investigated possible answers to our research questions according to the current state of literature.

  \item[YAML Subset] To determine a minimal usable subset of YAML we discussed common features required for a new Elektra storage plugin with some of the current developers as part of an presentation and subsequent discussion.

  \item[Implementation] First we implemented parsing code for the minimal YAML subset. In this phase we also added other necessary code to Elektra. After that we chose one of the parsing methods to create a more complete \glstext{YAML} parser.

  \item[Comparison] As noted in “\nameref{sec:Aim_Of_The_Work}” we compared the different implementations of our minimal YAML subset parsers.

\end{description}

\chapter{Background}

\section{State of the Art}

The book \citetitle{grune2007parsing}~\cite{grune2007parsing} provides a good overview of various up-to-date parsing algorithms. It covers the most popular techniques (such as LL- and LR-Parsing~\cite{knuth1965translation}) and also less well known methods up to 2007. The book also describes various classification possibilities for parsing techniques \cite[p. 85]{grune2007parsing}. The most common classification is the division into bottom-up and top-down parsers.

In \emph{top-down parsing} the parser starts with a hypothesis about the structure of the given data. The parser then tries to predict and match parts of the structure, starting from larger parts working its way down-to smaller elements.

We can further categorize parsing into \emph{directional} and \emph{non-directional} methods. Directional methods read the input from left to right, while non-directional methods can use an arbitrary order. This implies that directional methods are simpler and faster, but less powerful, than their non-directional counterparts. As part of this thesis we only consider directional methods, since they are faster and powerful enough to parse configuration data.

One of the most popular directional top-down methods is \emph{LL parsing}. While this technique is quite old – \citetitle{grune2007parsing} (p. 584) mentions a paper from 1961 belonging to the LL category – it is still actively used and researched. The basic idea behind LL parsing is simple: Begin with the start symbol of the grammar and the first character of the text. Then predict the next grammar rule, looking at the text to the right of the current position. We can categorize the technique further depending on the number of characters/tokens the parser uses to predict the next rule. If the parser uses one token of look-ahead we speak of an LL(1) parser, if it uses k tokens of lookahead we speak of an LL(k) parser~\cite{rosenkrantz1969properties}.

Two common methods to create an LL parser are:

\begin{enumerate}

  \item Implement the parser code using a set of mutually recursive procedures (Recursive Descent Parser). The code for this is either written by hand or produced by a parser generator such as \GlsShort{ANTLR}~\cite{parr2013recursive}.

  \item Use a parser generator to create a table-based parser.

\end{enumerate}

Examples of popular active projects that use a handwritten recursive descent parser include \href{http://clang.llvm.org}{clang}~\cite{bendersky2012clang} and \href{http://gcc.gnu.org}{GCC}~\cite{myers2008cparser}. The \href{http://www.antlr.org/about.html}{about page} for \gls{ANTLR} mentions some projects that use its generated recursive descent parsers. The list includes Twitter, wich uses ANTLR for query parsing and parsers for the languages used in the Apache Hadoop projects Hive and Pig~\cite{parr2013definitive}.

Some of the latest research developments in LL-parsing include LL(*) parsing~\cite{parr2011ll} and its successor Adaptive LL(*)~\cite{parr2014adaptive} (ALL(*)). Both of these algorithms use dynamic lookahead~\cite[p. 1]{parr2011ll}. While LL(*) parsing uses a static algorithm for rule prediction, ALL(*) analyses the input at run-time to improve prediction. As consequence of this parsers using the ALL(*) algorithm will be faster after an initial warm-up phase~\cite[p. 3]{parr2014adaptive}. LL(*) is part of \gls{ANTLR}~3~\cite[p. 3]{parr2014adaptive}, while \gls{ANTLR}~4 uses Adaptive LL parsing.


As we already mentioned before, the other popular parsing technique besides top-down-parsing is \emph{bottom-up parsing}. In bottom-up parsing the parser builds a structure starting with the smallest elements of the grammar (terminals). The parser then combines these elements into larger parts. One of the earliest entries in the \emph{linear bottom-up} parser category is the LR(k) parser~\cite{knuth1965translation}. Just like in LL(k) parsing, k specifies the number of lookahead symbols the parser uses.

\begin{sloppypar}
Unlike LL parsers, LR parsers are usually not created by hand, but generated by a parsing tool such as \href{https://www.gnu.org/software/bison}{bison} or \href{http://dinosaur.compilertools.net/yacc}{yacc}. Since LR(k) tables are very large, even for a small numbers of k, the parser tools mentioned before generate less powerful but smaller and faster LALR(k)~\cite{deremer1969practical} parsers.
\end{sloppypar}

LR(k) parsers are able to handle more grammars, than LL(k) parsers for the same constant k~\cite[section “Lookahead”]{haberman2013ll}. However, LR parsers are still not able to use ambiguous grammars. For this purpose \citeauthor{lang1974deterministic} describes the Generalized LR (GLR)~\cite{lang1974deterministic} method that is also able to handle these types of grammars. GLR parser are sometimes also called Tomita parsers~\cite{tomita1985efficient} after the author that described the first implementation of a generalized LR parser.

Recent research in the space of directional bottom-up parsing includes improved versions of techniques that are almost as powerful as canonical LR(1). One of the most promising method is IELR(1)~\cite{denny2008ielr}. The advantage of IELR(1) over LALR(1) is that it handles conflict resolution better. Parser tools such as bison use conflict resolution to handle non-LR grammars, i.e. grammars that contain rules where the parser is not able to decide what to do next. To handle these types of conflicts the grammar designer manually specifies which decision the parser should take. The current version of the parsing tool bison supports an experimental version of IELR(1).

Most parsing techniques can be categorized as either top-down or bottom-up. However, some techniques use a combination of both approaches. Others are usually not listed under one of the label top-down or bottom up, because they provide other features that the designer of these parsers deem more important, or they use features that do not fit well within either of these groups. In the remainder of this section we will discuss some of these techniques.

A method that can be categorized as either top-down technique with bottom-up recognition, or bottom-up technique with a top-down component~\cite[p. 206]{grune2007parsing} is \emph{Earley Parsing}~\cite{earley1970efficient}. Earley parsing is able to handle any context free grammar. This means the technique is as powerful as GLR parsing. This advantage comes at the cost of run-time. While LL parsing and LR parsing run in linear time depending on the length of the input ($O(n)$), Earley Parsing has an upper boundary of $O(n³)$. However, in \citeyear{leo1991general} \citeauthor{leo1991general} showed that an improved version of the algorithm handles most LR(k) grammars in linear time~\cite{kegler2011marpa, leo1991general, wikipedia2016Earley}. In \citeyear{aycock2002practical} \citeauthor{aycock2002practical} described improvements to the algorithm. Their version of Earley Parsing boosts the run-time in cases where the grammar contains nullable (empty) grammar rules. Recently \citeauthor{kegler2011marpa} incorporated the changes proposed by \citeauthor{leo1991general}, \citeauthor{aycock2002practical} in \href{http://savage.net.au/Marpa.html}{Marpa}~\cite{kegler2011marpa}.

All the methods we mentioned until now work with a description that is based on a (context-free) Chomsky grammar. These grammars describe a way to \emph{generate} words and sentences of a given language. Another way to specify the structure of a language is to give a description on how to \emph{recognize}~\cite[p. 506]{grune2007parsing} the words and sentences of a language. One popular recognition system are \glspl{PEG}. They were introduced by \citeauthor{ford2004parsing} in the paper \citetitle{ford2004parsing}~\cite{ford2004parsing}. \citeauthor{ford2002packrat} also describes how to write an efficient (top-down) parser that handles these types of grammars in linear time~\cite{ford2002packrat}. This method, called Packrat parsing, uses a specialized version of memoziation~\cite[p. 1]{ford2002packrat} to save intermediate results of the parsing process.

\begin{sloppypar}
Just like Packrat parsing, \emph{combinatory parsing}~\cite{frost1992constructing, hutton1992higher} specifies a method to create recursive descent parsers. As the name suggests, the focus in combinatory parsing is the composability of parsers. The technique is usually used in functional programming languages, such as Haskell. These languages support higher order functions, i.e. functions that take other functions as their parameters~\cite[p. 564]{grune2007parsing}. In combinatory parsing the parser creator typically starts by specifying parser (functions) for the simplest parts of the grammar (terminals). She or he then goes on to combine these simpler parsers into more powerful parsers for more complex rules (non-terminals). Combinatory parsing has similar problems as other top-down techniques such as LL parsing. One of these problems are left recursive grammar rules, i.e. rules that include references to themselves in the leftmost part of the right-hand side. Recently \citeauthor{frost2007modular} described a method to handle left recursive rules in combinatory parsing efficiently in the article \citetitle{frost2007modular}~\cite{frost2007modular}.
\end{sloppypar}

A method that is not a parsing technique per se, but a way to specify conversions of data from a source structure to a target structure and back is \emph{bidirectional programming}~\cite{foster2005combinators, bohannon2006relational}. The specification that allows this conversion is called a lens~\cite{foster2005combinators}. A programming language used to specify such lenses is Boomerang~\cite{bohannon2008boomerang}. The research of the Boomerang project lead to the creation of another project that uses lenses to parse configuration data: \href{http://augeas.net}{Augeas}. Augeas converts configuration data into a tree like representation. \citeauthor{berlakovich2016universal} implemented an Augeas plugin for Elektra as part of his Bachelor thesis~\cite{berlakovich2016universal}.

\chapter{Comparison}

In the evaluation phase of the work we concentrated on the criteria listed below.

\begin{description}

  \item[Run-time Performance]~\\[0.1cm]
  We used each of the parsing algorithms from the implementation phase and executed them using different input files. In each instance we measured the time it took to finish the parsing process. We repeated this multiple times for each combination of parsing technique and file, so we could determine median execution times. At the end of the phase we determined the fastest parsing code and therefore also answered the second scientific question: “Are hand-written parsers or generated parsers faster?”.

  \item[Memory Usage]~\\[0.1cm]
  We determined the memory usage of the different parsing implementation using \href{http://valgrind.org}{Valgrind's} heap profiler \href{http://valgrind.org/docs/manual/ms-manual.html}{Massif}.

  \item[Code Size]~\\[0.1cm]
  We counted the lines of the different implementations using the tool \href{https://github.com/AlDanial/cloc}{cloc}.

  \item[Code Complexity]~\\[0.1cm]
  For the measurement of the code complexity we determined
  \begin{itemize}
    \item the cyclomatic complexity~\cite{mccabe1976complexity}, and
    \item the Halstead complexity measures~\cite{halstead1977elements}
  \end{itemize}
  of the generated code.

  \item[Ease of Extensibility and Composability]~\\[0.1cm]
  Since there is no common way to measure either of this attributes we only looked into them from a more informal point of view. In this part of the evaluation we will describe some of the features we found that hinder or enhance the extensibility and composability of the used parsing methods. At this stage we will also answer the first scientific question, and argue why certain parsing libraries allow us to stay closer to the definition of the configuration language.

  \item[Error Reporting]~\\[0.1cm]
  For this subtask we created YAML input files, that contain certain errors. We will defined reference error messages that describe the problem in a way that is as user-friendly as possible. We then look how far we can approximate the ideal messages with the different parsing methods.

\end{description}

\backmatter

% -- Glossary & References -----------------------------------------------------

\printglossaries
\printbibliography

\end{document}
